\documentclass[runningheads,a4paper]{llncs}

\usepackage{cite}
\usepackage{array}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subfigure}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}
%\SweaveOpts{concordance=TRUE}

<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
library("ggthemes")
library("scales")
library(RColorBrewer)
measures.bf <- read.csv('data/op-measures-bitflip.csv')
measures.xo <- read.csv('data/op-measures-xover.csv')
measures.mo <- read.csv('data/measures-maxones.csv')
op.ratios <- read.csv('data/ops-ratios.csv')
op.ranks <- read.csv('data/ops-rank.csv')
ratios <- read.csv('data/ratios.csv')
ranks <- read.csv('data/ranks.csv')
@


\title{Ranking programming languages for evolutionary algorithm operations} 

% \author{\authorname{Juan-Julián Merelo-Guervós, Israel Blancas-Álvarez, 
%     Pedro A. Castillo, Gustavo Romero\sup{1}, Pablo Garc{\'ia}-S{\'a}nchez\sup{2}, 
%     Victor M. Rivas\sup{3}, 
%     Mario García-Valdez, Amaury Hernández-Águila\sup{4}, 
%     Mario Román\sup{5}}
% \affiliation{\sup{1}CITIC and Computer Architecture and Technology Department,  University of Granada (Spain),   Email: (jmerelo,iblancasa,pacv,gustavo)@ugr.es}
% \affiliation{\sup{2}Department of Computer Engineering, University of C\'adiz (Spain), Email: pablo.garciasanchez@uca.es}
% \affiliation{\sup{3}Department of Computer Sciences, University of Jaén (Spain), Email: vrivas@ujaen.es}
% \affiliation{\sup{4}Tijuana Institute of Technology (Mexico),Email: {amherag,mario}@tectijuana.edu.mx}\affiliation{\sup{5}University of Granada (Spain),Email: mromang08@correo.ugr.es }
% }

\author{A. U. Thorone, A. U. Thortwo, A. Uthorthree \and 
    A. Notherone \and 
    Thad Sit \and 
    Lars One }
\institute{Great Institute of Stuff \and
Univerity of Bestville \and
Research Center of Stuff \and
University of Evenbetter 
}

\maketitle

\keywords{Benchmarking, implementation of evolutionary algorithms,
  OneMax, genetic operators, programming languages, performance measurements.}

\begin{abstract}
  Despite the existence and popularity of many new and
  classical computer languages, the evolutionary algorithm 
community has mostly exploited a few popular ones, avoiding them, especially if they are not
compiled, under the asumption that compiled languages are {\em always}
faster than interpreted languages. Wide-ranging performance analyses
of implementation of evolutionary algorithms are usually focused on 
algorithmic implementation details and data structures, but these are usually limited to
specific languages. 
In this paper we measure the execution speed of three common operations in
genetic algorithms in many popular and
emerging computer languages using different data structures and
implementation alternatives, with several objectives: create a ranking for
these operations, compare relative speeds taking into account
different chromosome sizes and data structures, and dispel or show
evidence for several hypotheses that underlie most popular
evolutionary algorithm libraries and applications. We find that there
is indeed basis to consider compiled languages, such as Java, faster
in a general sense, but
there are other languages, including interpreted ones, that can hold
its ground against them.
\end{abstract}

\section{Introduction}

In the same spirit of the {\em No Free Lunch} theorem
\cite{Wolpert-1997-NFL} we could consider there is a {\em no fast
  lunch} \cite{2015arXiv151101088M} %Pablo: I do not consider necessary to anonymize this paper
hypothesis for the implementation of evolutionary optimization problems, in the
sense that, while there are particular languages that might be the
fastest for particular problem sizes and specially fitness functions
there is no single language that is the fastest for all chromosome
sizes, implementations and fitness functions. But the main problem is
that implementation decisions, and in many occasions reviewer reports,
are based on common beliefs such as thinking that a particular
language is the fastest or other language is too slow to even being
taken into consideration for implementing evolutionary algorithms.  
% sometimes the decision is done taken into account the language the user knows, and he does not try to learn new tools - Pedro

After initial tests on a smaller number of languages
\cite{DBLP:conf/evoW/MereloCBRGFRV16ANONYMOUS} and three different data
structures, in this paper we add more languages, different data
structures and also, in the case of languages with a particularly bad
result, new implementations including some made using 
released evolutionary algorithm libraries, and finally even 
found and corrected some bugs.  

To use the results we already had, we have re-used the same
operations: crossover, mutation and OneMax. In general
\cite{DBLP:conf/iwann/MereloRACML11} an Evolutionary Algorithm (EA) application will spend the most time running the fitness function and others, such as ranking the population; however, these are well covered by several 
general purpose benchmarks so they are not the focus of this paper. A priori, this result
would extend only to some implementations of genetic
algorithms. However, in this paper we would like to present not only
the result itself, which is interesting, but also a methodology to
first assess new languages for implementing evolutionary algorithms
for the value or insights they might give to the algorithm mechanism,
and second to make real-world measures and benchmark them to test their
%Pablo: second to take? Is that an expression? If not, I suggest to change it to "The second objective is to..."
speed and performance relative to other common languages instead of
choosing usual languages based only on past experience and common (maybe mis-) conceptions. 

The rest of the paper is organized as follows: coming up next in
Section \ref{sec:soa}, we will present the state of the art of the
analysis of EA implementations. Next we will present in
Section \ref{sec:exp} 
the tests we have used in this paper and its rationale along with the
languages we have chosen for carrying them out. Finally, in Section
\ref{sec:res} we 
will present the results obtained. Finally, we will draw the conclusions
%Pablo: question: should we differentiate OneMax (the problem) of onemax (the function)? If not, homogenize along the paper
%Mario: In this paper we are allways talking about the problem and its immplementation. I think they should be the same.
%If we talk about the function name we should use the tt style, but IMO in this paper we don't refer to any functions. 
and present future lines of work.

\section{State of the art}
\label{sec:soa}

The point of benchmarking evolutionary algorithms is to discover which
languages are more suitable for speedily running evolutionary
experiments. 

The
first published benchmarks of evolutionary algorithms \cite{jose1994genetic}
focused on implementation details using C and C++, while other papers
focused more on what kind of functions should be used to compare them
\cite{WHITLEY1996245}.

Despite the growing importance given to best practices in the software
creation process, there 
are not many publications on the subject, until recently when Alba et al. examined
the performance of different data structures, all of them using the
same language, in \cite{alba2007influence}. \cite{ae09}
described the implementation of an evolutionary algorithm in Perl,
also used in this paper, proving that, as a whole, Perl could run
evolutionary algorithms almost as fast as Java, but if we took
into consideration other factors like actual coding speed measured by
single lines of code, Perl was better. \cite{santana2011estimation}
focused on estimation of distribution algorithms, evaluating several
metaheuristic packages that have that particular one as an
option. However, it was not focused on speed, but on availability and
languages used. Its survey did uncover the fact that most software
packages used C or C++. 

Most papers, if not all, including this one, focus on single-threaded
procedural environments; for instance, a recent paper
\cite{nesmachnow2015empirical} focuses on a single language, C++;
despite this being a object-oriented language, this feature is not
taken into account in this particualr evaluation. In
these cases {\em classical} languages, optimized for the execution of
procedures and functions, not the dynamic creation of objects, have
a certain advantage. However, innovation in software engineering has not only spawned new
languages, but also new architectures like the Kappa architecture \cite{KappaArch}, 
microservices \cite{namiot2014micro} or
service-oriented frameworks 
\cite{garcia2010distributed,DBLP:journals/soco/Garcia-SanchezGCAG13} 
where we could envision that different parts of an evolutionary
algorithm might be written in different languages, but also in new,
domain specific, 
languages better suited for certain tasks. The {\em no fast lunch} principle
enunciated above implies that different languages could be used for
distributed systems, with the fastest or most appropriate language
used for every part of it. 
% If there is space we could put JavaScript 
%as an example of a language best suited for particular environment
%Also acknowledge some properties of popular langs like maturity, 
%standard libraries, trained pros etc. 
% See below
For instance, JavaScript is undoubtedly the best, if not the only,
language, that can be run natively in browsers; a volunteer
evolutionary computing system  such as the one described by Desell
\cite{desell2010analysis,desell2010validating} or Laredo
\cite{laredo2014designing} might leverage this fact to use mainly, or
exclusively, this particular language. A {\em polyglot} analysis of
languages for evolutionary algorithm such as the one done in this
paper will allow us to find out not only which languages are the
fastest in which environment, but also what is the actual difference
so that we can trade the speed difference for the convenience of using
a well-known tool ecosystem or a particular runtime environment. 
% maybe we could report the number of lines of code for each implementation?  - Pedro

All in all, existing literature focuses either on a single language
and different data structures or different, and mainly popular,
languages with a single data structure. In  previously published
research \cite{DBLP:conf/evoW/MereloCBRGFRV16ANONYMOUS} we focused on fewer
languages and mainly tried to measure their scaling behaviour across 
different lengths, and we found that Java, C\# and C obtained the best
results. However, we did not attempt to rank languages or measure
relative speeds across all tests.  This is what we do in this paper, extending the analysis of the previous paper with more languages and 
implementations. Next we will explain how the experiment was set up and
the functions used in it.

\section{Experimental setup}
\label{sec:exp}

We have used bitflip mutation, crossover and count-ones or OneMax in this
and the previous experiments, mainly since they are the
quintessential genetic algorithm operators and a benchmark used in
practical as well as theoretical approaches to Genetic Algorithms (GAs). 
In general, they exercise only a small part of the language capabilities,
involving mainly integer and memory-access performance through
loops. However, the implementation of these operations is deceptively
simple, specially for OneMax, a problem frequently used in
programming job interviews. 

Loops are also a key component in performance. Most languages allow
{\tt for} loops, however, many can also perform {\em map} implicit
loops where a function is run over each component of a data structure,
reducing sequential or random access to arrays. When available, we
have used these types of functions. This implies that
despite its simplicity, the results have wider applicability, except
for floating point performance, which is not tested, mainly because it
is a major component of many fitness functions, not so much of the
evolutionary algorithm itself. 

Chromosomes in EAs can be represented in several different ways: an
array or vector of Boolean values, or any other scalar value that can
be assimilated to it, or as a bitstring using generally ``1'' for true
values or ``0'' for false values. Different data structures will have
an impact on the result, since the operations that are applied to them
are, in many cases, completely different and thus the underlying
implementation is more or less efficient. Besides, languages use
different native data structures to represent this information. In
general, it can be divided into three different fields:\begin{itemize}
\item {\em Strings}: representing a set bit by 1 and unset by 0, it is
  a data structure present in all languages and simple to use in
  most.
\item {\em Vector of Boolean values}: not all languages have a
  specific primitive type for the Boolean false and true values; for
  those who have, sometimes they have specific implementations that
  make this data structure the most efficient. In some and when they
  boolean values were not available, 1 or 0 were used. {\em Bitsets}
  are a special case, using bits packed 
  into bytes for representing vector of bits, with 32 bits packed into a
  single 4 byte data structure and bigger number of bytes used as
  needed.
\item {\em Lists} are accessed only sequentially, although running
  loops over them might be more efficient that using random-access
  methods such as the ones above.
\end{itemize}

Besides, many languages, including functional ones, differentiate
between Mutable and Constant data structures, with different internal
representations assigned to every one of them, and extensive
optimizations used in Immutable or constant data structures. Immutable
data structures are mainly used in functional languages, but some
scripting languages like Ruby or Python use it for strings too. 

In this paper more than 20 languages, some of them with several implementations, 
have been chosen for performing all benchmarks; additionally, another language, 
Rust, has been tested for one of them, and Clojure using persistent vectors was 
tested only for OneMax. This list includes 9 languages from the top 10 in  
the TIOBE index \cite{tiobe16}, with Visual Basic the only one missing, and 1
more out of the top 20 (or two if we include Octave instead of the
proprietary application Matlab). The list of languages and alternative
implementations is shown in Table \ref{tab:files}. 

\begin{table*}[h!tb]
    \centering
    \begin{tabular}{l|c|l|l|l}
      \hline
      Language & Version & URL & Data structures & Type\\
      \hline %Pablo: replace ANONSERVER with git.io after acceptance!!!
      C & 4.8.2 & \url{http://ANONSERVER/v8T57} & Bit String & Compiled \\
      C++ & 4.8.4 & \url{http://ANONSERVER/v8T57} & Bit Vector  & Compiled\\
      C\# & mono 4.2  & \url{https://ANONSERVER/vzHDI} & Bit Vector  & Compiled\\
      Clojure & 1.8.0 & \url{https://ANONSERVER/vzHDe} & Bit Vector &  Compiled \\ 
      Common Lisp & 0.13.7 & \url{https://ANONSERVER/vzHyR} & Simple Bit Vector &  Compiled\\ 
      Go & go1.2.1 & \url{http://ANONSERVER/vBSYp} & Bit Vector & Compiled\\
      Haskell & ghc 7.10.3 & \url{https://ANONSERVER/vzHMw} & Mutable Vector & Compiled \\ 
      Java & 1.8.0\_66 & \url{http://ANONSERVER/v8TdR} & Bitset & Compiled\\
      JavaScript & node.js 5.0.0 & \url{http://ANONSERVER/vBSYd} & String & Interpreted\\
      Julia & 0.2.1 & \url{http://ANONSERVER/vBSOe} & Bit Vector & Interpreted \\
      Lua & 5.2.3 & \url{http://ANONSERVER/vBSY7} & String & Interpreted\\
      Octave & 3.8.1 & \url{http://ANONSERVER/v8T57} & BitVector & Interpreted \\ 
      PHP & 5.5.9 & \url{http://ANONSERVER/v8k9g} & String & Interpreted\\ % vrivas, 5-nov-2015
      Perl & v5.20.0 & \url{http://ANONSERVER/bperl} & String, Bit Vector & Interpreted \\
      Python & 2.7.3 & \url{http://ANONSERVER/vBSYb} & String & Interpreted\\
      Python 3 & 3.4.3 & \url{https://ANONSERVER/p3deap} & Bit Vector, List & Interpreted\\
      Rust &  1.4.0  & \url{https://ANONSERVER/EOr} & Bit Vector  & Compiled \\ 
      Scala & 2.11.7 & \url{http://ANONSERVER/vBSYH} & String, Bit Vector & Compiled \\
      Ruby & 1.9.3p551 & \url{https://ANONSERVER/rEO} & Bit Vector & Interpreted \\
      JRuby & 9.0.5.0 (2.2.3) & \url{https://ANONSERVER/rEO} & Bit Vector & Interpreted \\
      Free Pascal & 2.6.2-8 & \url{https://ANONSERVER/fpeo} & Bit Vector & Compiled \\
      Kotlin & 1.0.1 & \url{https://ANONSERVER/kEO} & Bit Vector &  Compiled \\
      Dart & 1.15.0 & \url{https://ANONSERVER/dEO} & List & Interpreted \\ 
      \hline
      \end{tabular}
      \vspace{5mm}
      \caption{Languages, versions, URLs, data structure and type of
        language used to carry out the
        benchmarks. No special flags were used for the interpreter or
        compiler. \label{tab:files}}
    \end{table*}
%

When available, open source implementations of the operators and OneMax were used.
In all cases except in Scala, implementation took less than one hour and was
inspired by the initial implementation made in Perl or in Lua. In
fact, we abandoned languages such as AWK or FORTRAN when it took too
long to make a proper implementation in them.
Adequate data and control
structures were used for running the application, which applies
mutation to a single generated chromosome a hundred thousand
times. The length of the mutated string starts at 16 and is doubled
until reaching $2^{15}$, that is, 32768. This upper length was chosen to
have an ample range, but also so small as to be able to run the
benchmarks within one hour. Results are shown next. In some cases and
when the whole test took less than one hour, length was taken up to
$2^{16}$.

In most cases, and especially in the ones where no implementation was
readily available, we wrote small programs with very little overhead
that called the functions directly. That means that using classes,
function-call chains, and other artifacts, will add an overhead to the
benchmark; besides, this implies that the implementation is not
exactly the same for all languages. However, this inequality reflects
what would be available for anyone implementing an evolutionary
algorithm and, when we think it might have an influence on the final
result, we will note it. 

Every program used also provides native capabilities for measuring
time, using system calls to check the time before and after operations
were performed. These facilities used the maximum resolution
available, which in some cases, namely Pascal, was somewhat inadequate.

All programs produced the same output, a comma separated set of values
that includes the language and data structure used, operand length and
time in seconds. 

\section{Results and analysis}
\label{sec:res}

%
\begin{figure}[h!tb]
  \centering
<<results-bf, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.bf$languagerepresentation))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
ggplot(measures.bf,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Bitflip")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))+theme(legend.position="bottom")+guides(colour = guide_legend(title.position = "bottom"))
@
\caption{Plot of the time needed to perform 100K mutations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale.}
\label{fig:time}
\end{figure}
%
\begin{figure}[h!tb]
  \centering
<<results-mo, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.mo$languagerepresentation))
ggplot(measures.mo,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Onemax")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))+theme(legend.position="bottom")+guides(colour = guide_legend(title.position = "bottom"))
@
\caption{Plot of time needed to perform 100K OneMax evaluations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale.}
\label{fig:time:mo}
\end{figure}
%


All the results have been made available in the repository that holds
this paper as well as some of the implementations, at
\url{https://ANONSERVER/bPPSN16}. %PABLO: replace ANONSERVER with git.io after acceptance!!!
 Implementation and results are available
with a free license. The Linux system we have used for testing runs the {\tt
  3.13.0-34-generic \#60-Ubuntu SMP} kernel on an {\tt Intel(R) Core(TM)
  i7-4770 CPU @ 3.40GHz} CPU. In this paper we will look mainly at the
aggregated results, comparing the differences in order of magnitude
between the different languages and also how they compare to each
other. 


\begin{figure*}[h!tb]
  \centering
<<results-ops-ratios,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot(op.ratios,aes(x=reorder(Language, -Ratio, FUN=median),Language,y=Ratio))+ geom_boxplot(notch=TRUE)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+scale_y_log10()+labs(x='Language')
@ 
\caption{Boxplot of scaled performance compared to the baseline
 language, which has been chosen to be Julia. Please note
that $y$ has a logarithmic scale. The strings indicate the language
and the implementation; for instance, Python\_DEAP\_numpy is a python
implementation using the operators in the DEAP framework \cite{fortin2012deap} and {\tt
  numpy} implementation for vectors. }
\label{fig:ops:ratios}
\end{figure*}
%
\begin{figure*}[h!tb]
  \centering
<<results-ratios,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot(ratios,aes(x=reorder(Language, -Ratio, FUN=median),Language,y=Ratio))+ geom_boxplot(notch=TRUE)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+scale_y_log10()+labs(x='Language')
@ 
\caption{Boxplot of scaled performance compared to baseline Julia. Please note
that $y$ has a logarithmic scale. The strings indicate the language
and the implementation; for instance, Python\_DEAP\_numpy is a python
implementation using the operators in the DEAP framework \cite{fortin2012deap} and {\tt
  numpy} implementation for vectors. }
\label{fig:ratios}
\end{figure*}
%
To have a general idea of performance and be able to compare across
benchmarks and sizes, we have used the language Julia, whose
performance is more or less in the middle,  as a baseline for comparison
and expressed all times as the ratio between the time needed for a
particular language and length and the speed for Julia. Ratios higher than 1 mean that the particular language+data
structure is faster than Julia, $<$1 the opposite. Please check the
boxplot in Figure \ref{fig:ratios} for the average and standard
deviation across all functions and lengths, when the comparison is
possible. 

We should first refer to the remarkable performance of Clojure using
persistent vectors. Clojure is a functional language, and immutable
structures lend themselves better to functional processing. Clojure
includes some specific functions for this kind of counting. However,
it should be noted that then we could not run Clojure on the rest of
the functions, bitflip and crossover. 

Next comes Java, with a performance that is around two orders of
magnitude better than baseline. In some cases, Clojure using mutable
vectors can be very fast too, but its overall performance takes it to
the third worst overall performance. This also implies that simply
choosing a language is not a guarantee of performance; data structures
chosen and implementation play also a major role. 

The top 5 is completed with C\#, Haskell and Scala, this last using
also a particular implementation, BitVector; if a more {\em natural}
BitString is used, its performance is on a par with the aforementioned
Clojure. Out of the top 5, three of them are functional languages:
Haskell, Clojure and Scala; Clojure is also an interpreted language,
with a JIT compiler that targets the Java Virtual Machine. Three of
them also use the Java Virtual Machine: Clojure, Java and Scala; C\#
has its own runtime too, with Haskell having a compiler to native
object code, being thus the only {\em pure} compiled language in this
set. 

The biggest differences reside among the top 3 languages. Haskell,
Scala, Go, C and Perl have a very similar performance, and it should
be remarked that Perl is the first interpreted language in the
list. Other compiled languages, including C++, have worse performance,
and the worst 10 include several compiled languages, as well as
functional languages such as clisp. Also Perl in a different
implementation, showing once again that a particular language, by
itself, need not be a guarantee of performance. 

From the best to the worst implementation, there are 4 orders of
magnitude of difference. This can be quite important because the range
of running time between the slowest and the fastest can go from 1
second to several hours. Languages such as Octave or clisp, maybe even
Python and Julia, should be avoided except for problems with a size
that allows them to be run in a few seconds. 

However, let us rank the languages looking at how they fare, comparing
with the other implementations. The averaged ranking shown in Figure
\ref{fig:ranks} averages the position reached for all sizes and
functions, and subtracts it from the total number of languages
tested, so that bigger is better.
%
\begin{figure*}[h!tb]
  \centering
<<results-ranks-ops,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot( op.ranks, aes(x=reorder(LanguageData,AvgRank), y=30-AvgRank))+ geom_bar(stat='identity',fill='orange',color='darkblue')+ theme_tufte() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(title='Average rank',x='Language+Data Structure',y='30-average rank')
@ 
\caption{Ranking averaged across all measures for genetic operators
  crossover and mutation, subtracted from the
  total number of measured languages, so bigger is better}
\label{fig:ranks:ops}
\end{figure*}
%
\begin{figure*}[h!tb]
  \centering
<<results-ranks,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot( ranks, aes(x=reorder(LanguageData,AvgRank), y=30-AvgRank))+ geom_bar(stat='identity',fill='orange',color='darkblue')+ theme_tufte() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(title='Average rank',x='Language+Data Structure',y='30-average rank')
@ 
\caption{Ranking averaged across all measures, subtracted from the
  total number of measured languages, so bigger is better}
\label{fig:ranks}
\end{figure*}
%
Even if on average and constrained to OneMax Clojure is better than
Java, this language beats it more times than the contrary, so it
becomes first in the ranking. C also achieves a better position than
in the previous graph, and Perl a worse position. Two emerging
languages which have been added for this paper, Kotlin and Dart,
together with JRuby, the implementation of Ruby for the JVM, close the
top 10 ranking. You need to look at this averaged 
rankings as the probability that a particular language is the best for
a particular function and size. Languages such as Pascal or Go are
more likely to win than JRuby or node. However, some groupings can
also be detected. The three best (Java, Clojure and C\#) form the
first, with a second group of three, up to Free Pascal, and then a gap
to \#7, which is Haskell. The worst 10, in this case, include two
compiled languages: C++ and Scala with a BitString implementation. The
rest are all interpreted languages. 

Another interesting result that can be observed in this ranking is the
domination of the Bit Vector structure over the rest, bearing in mind
that the BitSet used by Java is actually a type of bit vector with a
particular implementation. The best bit string implementation is C,
followed by Perl, which is actually very close. The best List
implementation, by the Dart language is in the middle region by
performance and the 9th by ranking. Let us discuss these findings in the next section, together with the
conclusions. 

\section{Conclusions}

In this paper we have measured the performance of an extensive
collection of languages in simple and common evolutionary algorithm
operations: mutation, crossover and OneMax, with the objective of
finding out which languages are faster at these operations and what
are the actual differences across languages, language types and data
structures, the main objective being that the EA practitioner can use
these results to make decisions over which language or languages to
use when implementing evolutionary languages. 

We can conclude that the language usually considered the fastest among
the practitioners, Java, indeed holds that position on average,
although it can be beaten by Clojure in some functions. Functional
languages have a remarkable performance, despite their lack of
popularity in the EA community. Using vector of bits rather than bit
strings or lists provides, in general, a better performance, and
immutable data structures, such as the ones used in functional
languages can be accessed and used, in general, faster than
mutable structures if available in the same language. 

Future lines of work might include a more extensive measurement of
other operators such as  tournament selection and other selection
algorithms. A priori, these are essentially CPU integer
operations and their behavior might be, in principle, very similar to 
the one shown in these operations. It would also be interesting to mix and match
different languages, choosing every one for its performance, in a hybrid
architecture. Communication might have some overhead, but it might be
offset by performance. Combining some compiled languages such as Go or
C with others characterized by its speed in some string operations,
like Perl or programming ease, like Python, might result in the best of both worlds:
performance and rapid prototyping. Creating a whole multi-language
framework along these lines is a challenge that might be interesting
in the future. 

Besides, in some cases the languages have not been used to their full
potential. Concurrent languages such as Scala or Go are actually used
sequentially, missing features that a priori would make them stand out
over languages not designed with that feature, such as Java. 

The full set of languages and tests will also be made available as a
Docker container, which can be downloaded easily to run it in particular environments  % maybe a reference to Docker should be added?  - Pedro
and machines.

\section*{Acknowledgements}

 % This paper is part of the open science effort at the university of
 % Granada. It has been written using {\tt knitr}, and its source as well as
 % the data used to create it can be downloaded from 
 % \href{https://github.com/JJ/2016-ea-languages-PPSN}{the GitHub
 %   repository} \url{https://github.com/geneura-papers/2016-ea-languages-PPSN/}. 
 % It has been supported in part by
 % \href{http://geneura.wordpress.com}{GeNeura Team}, 
 % projects TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitiveness),
 % Conacyt Project PROINNOVA-220590.  
Acks\\
Taking\\
This\\
Much\\
Space


\bibliographystyle{apalike}
\bibliography{geneura,languages,GA-general}

\end{document}

%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% hunspell-local-dictionary: "english"
%%% End:
