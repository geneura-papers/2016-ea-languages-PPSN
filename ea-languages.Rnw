\documentclass[runningheads,a4paper]{llncs}

\usepackage{cite}
\usepackage{array}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subfigure}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
library("ggthemes")
library("scales")
library(RColorBrewer)
measures.bf <- read.csv('data/op-measures-bitflip.csv')
measures.xo <- read.csv('data/op-measures-xover.csv')
measures.mo <- read.csv('data/measures-maxones.csv')
op.ratios <- read.csv('data/ops-ratios.csv')
op.ranks <- read.csv('data/ops-rank.csv')
ratios <- read.csv('data/maxones-ratios.csv')
ranks <- read.csv('data/maxones-ranks.csv')
@


\title{Ranking programming languages for evolutionary algorithm operations} 

% \author{\authorname{Juan-Julián Merelo-Guervós, Israel Blancas-Álvarez, 
%     Pedro A. Castillo, Gustavo Romero\sup{1}, Pablo Garc{\'ia}-S{\'a}nchez\sup{2}, 
%     Victor M. Rivas\sup{3}, 
%     Mario García-Valdez, Amaury Hernández-Águila\sup{4}, 
%     Mario Román\sup{5}}
% \affiliation{\sup{1}CITIC and Computer Architecture and Technology Department,  University of Granada (Spain),   Email: (jmerelo,iblancasa,pacv,gustavo)@ugr.es}
% \affiliation{\sup{2}Department of Computer Engineering, University of C\'adiz (Spain), Email: pablo.garciasanchez@uca.es}
% \affiliation{\sup{3}Department of Computer Sciences, University of Jaén (Spain), Email: vrivas@ujaen.es}
% \affiliation{\sup{4}Tijuana Institute of Technology (Mexico),Email: {amherag,mario}@tectijuana.edu.mx}\affiliation{\sup{5}University of Granada (Spain),Email: mromang08@correo.ugr.es }
% }

\author{A. U. Thorone, A. U. Thortwo, A. Uthorthree \and 
    A. Notherone \and 
    Thad Sit \and 
    Lars One }
\institute{Great Institute of Stuff \and
Univerity of Bestville \and
Research Center of Stuff \and
University of Evenbetter 
}

\maketitle

\keywords{Benchmarking, implementation of evolutionary algorithms,
  OneMax, genetic operators, programming languages, performance measurements.}

\begin{abstract}
In this paper we measure the speed of several popular and recent
programming languages performing the most usual operators in the
canonical evolutionary algorithm, mutation and crossover, as well as
an usual fitness function, OneMax, which is representative of the kind
of operations performed in binary chromosomes. Our main objective is,
first, to find, for each language which is the fastest data structure
%Pablo: Although this phrase is easily understood, structures are not fast per se (the operations are)
available. Second, to find out the ranges of variation of speeds for
%Pablo: what is a range of variation here?
the different languages. Third, to find out whether the usual
assumptions about the speed of languages really holds. And, finally, to
find if the assumed order of speed in languages used in evolutionary
algorithms holds true for all kinds of operations. In order to do that,
we use available implementations or perform our own, concluding that
the evolutionary algorithm scenario is more complex than usually
assumed and finding out some surprising {\em winners} and {\em losers} among the
languages tested. 
\end{abstract}

\section{Introduction}

In the same spirit of the {\em No Free Lunch} theorem
\cite{Wolpert-1997-NFL} we could consider there is a {\em no fast
  lunch} \cite{2015arXiv151101088M} %Pablo: I do not consider necessary to anonymize this paper
hypothesis for the implementation of evolutionary optimization problems, in the
sense that, while there are particular languages that might be the
fastest for particular problem sizes and specially specific fitness functions
there is no single language that is the fastest for all chromosome
sizes, implementations, which include language versions, and fitness
functions. That being the case, implementation decisions, and in many
occasions reviewer reports, 
are based on common beliefs such as thinking that a particular
language is the fastest or that some other language is too slow for even being
taken into consideration for implementing evolutionary algorithms. In
many cases, that decision is done taken into account the language the
user knows, and he does not try to learn new tools. 

In fact, the current state of the art in application development
eschews monolithic applications for loosely coupled set of processing
components, every one of them possibly written in a different
language and running on the cloud
\cite{wu2016least,zhang2008evolutionary,swann2015towards}. In that
scenario, the 
possibility and even the likelihood of 
multiple parts of an evolutionary algorithm written in different
languages, every one of them suited to a particular component of the
evolutionary algorithm, becomes higher. Besides, as new languages
become more familiar, it is interesting to have an estimation of their
speed when running classical evolutionary algorithm
operations. Besides, these asynchronous evolutionary algorithms
with polyglot components might insert a component of asynchrony
\cite{scott2015understanding,jj:2008:PPSN} that may have a positive
impact on the overall performance of the algorithm. 

After initial tests on a smaller number of languages 
\cite{DBLP:conf/evoW/MereloCBRGFRV16ANONYMOUS} and several different data
structures, in this paper we test new languages, different data
structures, new implementations -including some made using 
released evolutionary algorithm libraries-, found and corrected some
%Pablo: I added the - instead using parentheses, as the phrase is too long
bugs and also updated language versions in the case new, and faster,
ones have been released. 
%Pablo: I would move the next paragraph to experimental section and remove the "usually"
In the case a non-current version is shown it
is usually because we have tested the new versions and kept the
fastest one, which happened to be the last one. 


In order to make a fair comparison to the results we already had, we have re-used the same
operations: crossover, mutation and OneMax. However, we have made an
analysis that takes crossover and mutation in one hand and OneMax in
another, since our preliminary results showed that there was sometimes
a big difference between the speeds and their scale up found in one or
the other. 

We have used just these functions instead of implementing a whole
evolutionary algorithm, first because it would have been impossible to
achieve this breadth of languages and, second, because these
operations, in fact, determine the total speed of an evolutionary
algorithm program; indeed, it might be the case that
\cite{DBLP:conf/iwann/MereloRACML11} an Evolutionary Algorithm (EA)
application will spend the most time running the fitness function and
others, such as ranking the population; however, these are well
covered by several  
general purpose benchmarks so they are not the focus of this paper. 

A priori, results obtained here would generalize only to some
implementations of genetic 
algorithms. However, in this paper we would like to present not only
the result itself, which is interesting, but also a methodology to
first assess new languages for implementing evolutionary algorithms
for the value or insights they might give to the algorithm mechanism.
Additionally, to make real-world measures and benchmark them to test their
speed and performance relative to other common languages instead of
choosing usual languages based only on past experience and common (maybe mis-) conceptions. 
%Pablo: too large paragraph, I divided it in two

The rest of the paper is organized as follows: coming up next in
Section \ref{sec:soa}, we will present the state of the art of the
analysis of EA implementations. Next we will present in
Section \ref{sec:exp} 
the tests we have used in this paper and its rationale along with the
languages we have chosen for carrying them out. Then, in Section
\ref{sec:res} we 
will present the results obtained. Finally, we will draw the conclusions
and present future lines of work.

\section{State of the art}
\label{sec:soa}

The point of benchmarking evolutionary algorithms is to discover which
languages are more suitable for speedily running evolutionary
experiments. 

The
first published benchmarks of evolutionary algorithms \cite{jose1994genetic}
focused on implementation details using C and C++, while other papers
focused more on what kind of functions should be used to compare them
\cite{WHITLEY1996245}.

Despite the growing importance given to best practices in the software
creation process, there 
are not many publications on the subject, until recently when Alba et al. examined
the performance of different data structures, all of them using the
same language, in \cite{alba2007influence}. \cite{ae09}
described the implementation of an evolutionary algorithm in Perl,
also used in this paper, proving that, as a whole, Perl could run
evolutionary algorithms almost as fast as Java, but if we took
into consideration other factors like actual coding speed measured by
single lines of code, Perl was better. \cite{santana2011estimation}
focused on estimation of distribution algorithms, evaluating several
metaheuristic packages that have that particular one as an
option. However, it was not focused on speed, but on availability and
languages used. Its survey did uncover the fact that most software
packages used C or C++. 

Most papers, if not all, including this one, focus on single-threaded
procedural environments; for instance, a recent paper
\cite{nesmachnow2015empirical} focuses on a single language, C++;
despite this being a object-oriented language, this feature is not
taken into account in this particular evaluation. In
these cases {\em classical} languages, optimized for the execution of
procedures and functions, not the dynamic creation of objects, have
a certain advantage. However, innovation in software engineering has not only spawned new
languages, but also new architectures like the Kappa architecture \cite{KappaArch}, 
microservices \cite{namiot2014micro} or
service-oriented frameworks 
\cite{garcia2010distributed,DBLP:journals/soco/Garcia-SanchezGCAG13} 
where we could envision that different parts of an evolutionary
algorithm might be written in different languages, but also in new,
domain specific, 
languages better suited for certain tasks. The {\em no fast lunch} principle
enunciated above implies that different languages could be used for
distributed systems, with the fastest or most appropriate language
used for every part of it. 
% If there is space we could put JavaScript 
%as an example of a language best suited for particular environment
%Also acknowledge some properties of popular langs like maturity, 
%standard libraries, trained pros etc. 
% See below
For instance, JavaScript is undoubtedly the best, if not the only,
language, that can be run natively in browsers; a volunteer
evolutionary computing system  such as the one described by Desell
\cite{desell2010analysis,desell2010validating} or Laredo
\cite{laredo2014designing} might leverage this fact to use mainly, or
exclusively, this particular language. A {\em polyglot} analysis of
languages for evolutionary algorithm such as the one done in this
paper will allow us to find out not only which languages are the
fastest in which environment, but also what is the actual difference
so that we can trade the speed difference for the convenience of using
a well-known tool ecosystem or a particular runtime environment. 
% maybe we could report the number of lines of code for each implementation?  - Pedro
% We would have to measure them. What's the point? - JJ

All in all, existing literature focuses either on a single language
and different data structures or different, and mainly popular,
languages with a single data structure. In  previously published
research \cite{DBLP:conf/evoW/MereloCBRGFRV16ANONYMOUS} we focused on fewer
languages and mainly tried to measure their scaling behaviour across 
different lengths, and we found that Java, C\# and C obtained the best
results. However, we did not attempt to rank languages or measure
relative speeds across all tests.  This is what we do in this paper,
extending the analysis of the previous paper with more languages and
different implementations. Next we will explain how the experiment was set up and
the functions used in it.

\section{Experimental setup}
\label{sec:exp}

The main intention of this paper was to test new languages on these
genetic operators, so we have had to program from scratch these
functions, following in many cases instructions from experts so that
we managed to extract the maximum speed out of the data and control
structures for the particular language. As mentioned above, crossover,
mutation are the most widely used  operators for binary evolutionary
algorithms; OneMax is also a widely used fitness function and one
whose operations are representative or most binary fitness functions. 
In general, they exercise only a small part of the capabilities of the
language,
involving mainly integer and memory-access performance through
loops. Their implementation of these operations is deceptively
simple, specially for OneMax, a problem frequently used in
programming job interviews but whose fastest implementation fully
depends on the language. 

Any of these operations use loops. Most languages include
{\tt for} loops; many can also perform {\em map} implicit
loops where a function is applied to every component of a data structure,
reducing sequential or random access to arrays. Other, more complex
operations, such as {\em reduce}, which apply functions sequentially
to all members of an array accumulating over an initial value, are
sometimes available too.
When available, we have used them. This implies that
despite the simplicity of the benchmarking functions chosen, the
results have wider applicability, % change this if we include Griewank
                                  % - JJ
except 
for floating point performance, which is not tested, mainly because it
is a major component of many fitness functions, not so much of the
evolutionary algorithm itself. 

Data structures are the subject of these control structures; in this
case we are dealing with a single one, the {\em chromosome}, which is
widely accepted to be a list of zeros and ones. However, there are
many possible ways of interpreting the meaning of ``list'' and of
``zero'' and ``one''. 

Let us pay attention first to the structure itself, the {\em
  list}. In general, it will need to be a data structure that uses
sequential access, a list or an array. However, for some fitness
functions such as the one we are using here, a simple set will
do. Mutation will be changing a single element in the set, crossover a
group of elements. On the other hand, the ``present''/''absent''
concept can be represented by three type of atomic scalar data: a
character (which is usually 1 or 0), an integer (1 or 0) or a Boolean
value (true or false). Depending o the data structure chosen, we might
have to choose a particular representation or not. 
The data structures we are actually using in this
study can be divided into three different fields:\begin{itemize}
\item {\em Strings}: representing a set bit by ``1'' and unset by ``0'', it is
  a data structure present in all languages and simple to use in
  most. In general, strings are used for its legibility and also the
  memory efficiency of storing values contiguosly. However, it need
  not be the most efficient one when computing fitness.
\item {\em Vectors of Boolean values}, or Bit Vectors: not all languages have a
  specific primitive type for the Boolean false and true values; for
  those who have, sometimes they have specific implementations that
  make this data structure the most efficient. In some and when they
  Boolean values were not available, 1 or 0 were used. {\em Bitsets}
  are a special case, using bits packed 
  into bytes for representing vector of bits, with 32 bits packed into a
  single 4 byte data structure and bigger number of bytes used as
  needed.
\item {\em Lists} are accessed only sequentially, although running
  loops over them might be more efficient that using random-access
  methods such as the ones above. {\em Sequences} are functional data
  structures that differ from Lists in their finite size and the set
  of operations they are optimized for. 
\end{itemize}

Besides, many languages, including functional ones, differentiate
between Mutable and Constant or Immutable data structures, with different internal
representations assigned to every one of them, and extensive
optimizations used in Immutable or constant data structures. Immutable
data structures are mainly used in functional languages, but some
scripting languages like Ruby or Python use it for strings too. When
available, we have also tested this type of data structures. 

In this paper we have run the benhmarks in 22 different languages; additionally, another language, 
Rust, has been tested for one of them, and Clojure using persistent vectors was 
tested only for OneMax. This list includes 9 languages from the top 10 in  
the TIOBE index \cite{tiobe16}, with Visual Basic the only one
missing, and 10 out of the top 20\footnote{Considering that Octave and
  Matlab actually use the same language. We have not considered
  proprietary implementations of languages such as that one). The list of languages and alternative
implementations is shown in Table \ref{tab:files}. 

\begin{table*}[h!tb]
    \centering
    \begin{tabular}{l|c|l|l|l}
      \hline
      Language & Version & URL & Data structures & Type\\
      \hline %Pablo: replace ANONSERVER with git.io after acceptance!!!
      C & 4.8.2 & \url{http://ANONSERVER/v8T57} & String & Compiled \\
      C++ & 4.8.4 & \url{http://ANONSERVER/v8T57} & Bit Vector  & Compiled\\
      C\# & mono 4.2  & \url{https://ANONSERVER/vzHDI} & Bit Vector  & Compiled\\
      Clojure & 1.8.0 & \url{https://ANONSERVER/vzHDe} & Bit Vector &  Compiled \\ 
      Common Lisp & 0.13.7 & \url{https://ANONSERVER/vzHyR} & Simple Bit Vector &  Compiled\\ 
      Dart & 1.15.0 & \url{https://ANONSERVER/dEO} & List & Interpreted \\ 
      Go & go1.5.3 & \url{http://ANONSERVER/vBSYp} & Bit Vector & Compiled\\
      Haskell & ghc 7.10.3 & \url{https://ANONSERVER/vzHMw} & Mutable Vector & Compiled \\ 
      Java & 1.8.0\_66 & \url{http://ANONSERVER/v8TdR} & Bitset & Compiled\\
      JavaScript & node.js 5.0.0 & \url{http://ANONSERVER/vBSYd} & String & Interpreted\\
      JRuby & 9.0.5.0 (2.2.3) & \url{https://ANONSERVER/rEO} & Bit Vector & Interpreted \\
      Julia & 0.2.1 & \url{http://ANONSERVER/vBSOe} & Bit Vector & Interpreted \\
      Kotlin & 1.0.1 & \url{https://ANONSERVER/kEO} & Bit Vector &  Compiled \\
      Lua & 5.3.3 & \url{http://ANONSERVER/vBSY7} & String & Interpreted\\
      Octave & 3.8.1 & \url{http://ANONSERVER/v8T57} & BitVector & Interpreted \\ 
      Pascal & Free Pascal 2.6.2-8 & \url{https://ANONSERVER/fpeo} & Bit Vector & Compiled \\
      PHP & 5.5.9 & \url{http://ANONSERVER/v8k9g} & String & Interpreted\\ % vrivas, 5-nov-2015
      Perl & v5.24.0 & \url{http://ANONSERVER/bperl} & String & Interpreted \\
      Perl6 & 2016.09.1 & \url{http://ANONSERVER/bperl6} & String, Bit Vector & Interpreted \\
      Python & 2.7.3 & \url{http://ANONSERVER/vBSYb} & String & Interpreted\\
      Python 3 & 3.4.3 & \url{https://ANONSERVER/p3deap} & Bit Vector, List & Interpreted\\
      Ruby & 1.9.3p551 & \url{https://ANONSERVER/rEO} & Bit Vector & Interpreted \\
      Rust &  1.4.0  & \url{https://ANONSERVER/EOr} & Bit Vector  & Compiled \\ 
      Scala & 2.11.7 & \url{http://ANONSERVER/vBSYH} & String, Bit Vector & Compiled \\
      \hline
      \end{tabular}
      \vspace{5mm}
      \caption{Languages, versions, URLs, data structures and type of
        language used to carry out the
        benchmarks. No special flags were used for the interpreter or
        compiler. \label{tab:files}}
    \end{table*}
%

When available, available open source implementations of the operators
and OneMax were used. In most cases, implementation took less than one hour and was
inspired by the initial implementation made in Perl or in Lua;
however, finetuning and improving from the last version published, as
well as the testing of several versions of the language to choose the
fastest, took longer. Adequate data and control
structures were used for running the application, which applies
mutation to a single generated chromosome, or pair of them, a hundred thousand
times. The length of the mutated string starts at 16 and is doubled
until reaching $2^{15}$, that is, 32768; in some cases we stopped at
$2^{14}$ if it took more than one hour; we also extended it in some
cases to $2^{16}$. 

In most cases, and especially in the ones where no implementation was
readily available, we wrote small programs with very little overhead
that called the functions directly. Using classes,
function-call chains, and other artifacts, will add an overhead to the
benchmark and will need to be different for each language. However,
legibility or other constraints will make us use it anyway; this will
have an influence in performance, but the differences  will reflects
what would be available for anyone implementing an evolutionary
algorithm in a new language. 

Every program uses native functions for measuring time, using system calls to check the time before and after operations
were performed. These facilities used the maximum resolution
available, which in some cases, namely Pascal, was somewhat
inadequate. This was preferred to using external facilities since
loading times, which are not important when running long experiments,
will be different for compiled and interpreted languages; besides,
external time will be more influenced by things like the hard drive
state or other programs running.

\section{Results and analysis}
\label{sec:res}

%
\begin{figure}[h!tb]
  \centering
<<results-bf, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.bf$languagerepresentation))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
ggplot(measures.bf,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Bitflip")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))+theme(legend.position="bottom")+guides(colour = guide_legend(title.position = "bottom"))
@
\caption{Plot of the time needed to perform 100K mutations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale. Every language key includes the data structure used; it will be maintained in the rest of the paper.}
\label{fig:time}
\end{figure}
%
\begin{figure}[h!tb]
  \centering
<<results-mo, cache=FALSE,echo=FALSE>>=
colourCount = length(unique(measures.mo$languagerepresentation))
ggplot(measures.mo,aes(x=length,y=time,colour=factor(languagerepresentation)))+ geom_line() + geom_point() +  ggtitle("Evolutionary algorithm language benchmarks: Onemax")+scale_y_log10()+scale_x_log10()+scale_color_manual(name='Language',values=getPalette(colourCount))+theme(legend.position="bottom")+guides(colour = guide_legend(title.position = "bottom"))
@
\caption{Plot of time needed to perform 100K OneMax evaluations in strings with
lengths increasing by a factor of two from 16 to $2^{15}$. Please note
that $x$ and $y$ both have a logarithmic scale.}
\label{fig:time:mo}
\end{figure}
%

All the results have been made available, with a free license, in the repository that holds
this paper as well as some of the implementations, at
\url{https://ANONSERVER/bPPSN16}. %PABLO: replace ANONSERVER with git.io after acceptance!!!
The Linux system we have used for testing runs the {\tt
 3.13.0-34-generic \#60-Ubuntu SMP} kernel on an {\tt Intel(R) Core(TM)
  i7-4770 CPU @ 3.40GHz} CPU. Results for bitflip mutation and OneMax are shown in Figures \ref{fig:time} and \ref{fig:time:mo}. Crorsover results are not shown for brevity. They are quite similar to what is shown in \ref{fig:time}. These figures mainly show how time needed to perform varies more or less linearly with size; but in the case of some languages, the fact that they do not make them show a stronger performance in the long run. The case of Java is really remarkable, in fact. We will examine aggregated results next.


\begin{figure*}[h!tb]
  \centering
<<results-ops-ratios,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot(op.ratios,aes(x=reorder(Language, -Ratio, FUN=median),Language,y=Ratio))+ geom_boxplot(notch=TRUE)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+scale_y_log10()+labs(x='Language')
@ 
\caption{Boxplot of scaled performance compared to the baseline
 language, which has been chosen to be Julia. Please note
that $y$ has a logarithmic scale. The strings indicate the language
and the implementation; for instance, Python\_DEAP\_numpy is a python
implementation using the operators in the DEAP framework \cite{fortin2012deap} and {\tt
  numpy} implementation for vectors. }
\label{fig:ops:ratios}
\end{figure*}
%
\begin{figure*}[h!tb]
  \centering
<<results-ratios,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot(ratios,aes(x=reorder(Language, -Ratio, FUN=median),Language,y=Ratio))+ geom_boxplot(notch=TRUE)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+scale_y_log10()+labs(x='Language')
@ 
\caption{Boxplot of scaled performance compared to baseline Julia. Please note
that $y$ has a logarithmic scale. }
\label{fig:ratios}
\end{figure*}
%
To have a general idea of performance and be able to compare across
benchmarks and sizes, we have used the language Julia, whose
performance is more or less in the middle, as a baseline for comparison
and expressed all times as the ratio between the time needed for a
particular language and length and the time it takes the equivalent
program in Julia. Ratios higher than 1 mean that the particular
language+data   
structure is faster than Julia, $<$1 the opposite. The
boxplot in Figure \ref{fig:ratios:op} shows the average and standard
deviation for all available lengths for the genetic operators; \ref{fig:ratios} shows the same for Onemax. 

% Comment results here

However, let us rank the languages looking at how they fare, comparing
with the other implementations. The averaged ranking for genetic operators is shown in Figure
\ref{fig:ranks:ops} and for OneMax in \ref{fig:ranks} averages the position reached for all sizes and
functions, and subtracts it from the total number of languages
tested, so that bigger is better.
%
\begin{figure*}[h!tb]
  \centering
<<results-ranks-ops,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot( op.ranks, aes(x=reorder(LanguageData,AvgRank), y=30-AvgRank))+ geom_bar(stat='identity',fill='orange',color='darkblue')+ theme_tufte() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(title='Average rank',x='Language+Data Structure',y='30-average rank')
@ 
\caption{Ranking averaged across all measures for genetic operators
  crossover and mutation, subtracted from the
  total number of measured languages, so bigger is better}
\label{fig:ranks:ops}
\end{figure*}
%
\begin{figure*}[h!tb]
  \centering
<<results-ranks,message=FALSE, cache=FALSE,echo=FALSE>>=
ggplot( ranks, aes(x=reorder(LanguageData,AvgRank), y=30-AvgRank))+ geom_bar(stat='identity',fill='orange',color='darkblue')+ theme_tufte() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(title='Average rank',x='Language+Data Structure',y='30-average rank')
@ 
\caption{Ranking averaged across all measures, subtracted from the
  total number of measured languages, so bigger is better}
\label{fig:ranks}
\end{figure*}
%

% Comment results

\section{Conclusions}

In this paper we have measured the performance of an extensive
collection of languages in simple and common evolutionary algorithm
operations: mutation, crossover and OneMax, with the objective of
finding out which languages are faster at these operations and what
are the actual differences across languages, language types and data
structures, the main objective being that the EA practitioner can use
these results to make decisions over which language or languages to
use when implementing evolutionary languages. 

%Rewrite conclusions

Future lines of work might include a more extensive measurement of
other operators such as  tournament selection and other selection
algorithms. A priori, these are essentially CPU integer
operations and their behavior might be, in principle, very similar to 
the one shown in these operations. It would also be interesting to mix and match
different languages, choosing every one for its performance, in a hybrid
architecture. Communication might have some overhead, but it might be
offset by performance. Combining some compiled languages such as Go or
C with others characterized by its speed in some string operations,
like Perl or programming ease, like Python, might result in the best of both worlds:
performance and rapid prototyping. Creating a whole multi-language
framework along these lines is a challenge that might be interesting
in the future. 


\section*{Acknowledgements}

 % This paper is part of the open science effort at the university of
 % Granada. It has been written using {\tt knitr}, and its source as well as
 % the data used to create it can be downloaded from 
 % \href{https://github.com/JJ/2016-ea-languages-PPSN}{the GitHub
 %   repository} \url{https://github.com/geneura-papers/2016-ea-languages-PPSN/}. 
 % It has been supported in part by
 % \href{http://geneura.wordpress.com}{GeNeura Team}, 
 % projects TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitiveness),
 % Conacyt Project PROINNOVA-220590.  
Acks\\
Taking\\
This\\
Much\\
Space


\bibliographystyle{apalike}
\bibliography{geneura,languages,GA-general}

\end{document}

%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% hunspell-local-dictionary: "english"
%%% End:
